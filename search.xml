<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Redis 过期key删除策略和内存淘汰策略]]></title>
    <url>%2Freading%2Fredis_memory%2F</url>
    <content type="text"><![CDATA[过期删除策略当Redis中缓存的key过期了，Redis如何处理？ 惰性删除在客户端访问 key 的时候，如果这个 key 设置了过期时间，redis 则对 key 的过期时间进行检查，如果过期了就立即删除。memcached只是用了惰性删除，而Redis同时使用了惰性删除与定期删除 定期扫描删除redis 会将每个设置了过期时间的 key 放入到一个独立的字典中，然后会定时遍历这个字典来删除到期的 key。这个定时任务由 src/expire.c 的 activeExpireCycle(ACTIVE_EXPIRE_CYCLE_SLOW) 函数来完成，它的大致步骤是： 每次从过期字典中随机选取20个key。 删除这20个key中过期的key。 如果这20个key中过期key的比例超过了 25%，则重复步骤1。 123src/server.h 中定义的每次获取 key 的数量和比例。#define ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP 20 /* Loopkups per loop. */#define ACTIVE_EXPIRE_CYCLE_SLOW_TIME_PERC 25 /* CPU max % for keys collection */ 为了防止这个过期扫描出现循环过度，此算法加了一个timelimit变量，函数每次默认最多执行25ms 1234567891011//默认情况下，timelimit=25000(纳秒)timelimit = 1000000*ACTIVE_EXPIRE_CYCLE_SLOW_TIME_PERC/server.hz/100; if ((iteration &amp; 0xf) == 0) &#123; /* check once every 16 iterations. */ //删除过期key，每迭代16次，检查一次timelimit elapsed = ustime()-start; //ustime()精确到纳秒 if (elapsed &gt; timelimit) &#123; timelimit_exit = 1; break; &#125;&#125; 即使加了timelimit，默认每100ms执行一次activeExpireCycle。如果同一时间有大量key过期，在默认情况下 Redis 也会有至少 1 / 4 的时间被阻塞来执行这个activeExpireCycle()方法。为防止出现这种情况，可以在设置过期时间时 后面再个随机值： 12// 在目标过期时间上增加一天的随机时间redis.expire_at(key, random.randint(24*3600) + expire_ts) 从库的过期策略从库不会进行过期扫描，从库对过期的处理是被动的。主库在 key 到期时，会在 AOF 文件里增加一条 del 指令，同步到所有的从库，从库通过执行这条 del 指令来删除过期的 key。因为指令同步是异步进行的，所以主库过期的 key 的 del 指令没有及时同步到从库的话，会出现主从数据的不一致，主库没有的数据在从库里还存在，比如在集群环境中分布式锁的算法漏洞就是因为这个同步延迟产生的。 单线程的 Redis，如何知道要运行定时任务？这个定期扫描删除任务在由 server.c/serverCron() 方法调用，Redis 将 serverCron 方法称作“timer interrupt”（循环时间事件），它每秒的执行次数可以通过redis.conf hz选项（默认是10）来配置。serverCron() 除了定期扫描删除过期key，主要还执行： 更新服务器的各类统计信息，比如时间（server.lruclock）、内存占用、数据库占用情况等。 关闭和清理连接失效的客户端。 尝试进行 AOF 或 RDB 持久化操作。 如果服务器是主节点的话，对附属节点进行定期同步。 如果处于集群模式的话，对集群进行定期同步和连接测试。 内存淘汰策略当Redis已用内存超过maxmemory限定时，怎么处理需要新写入且需要申请额外空间的数据？根据redis.conf maxmemory-policy 选项来做对应的清理： volatile-lru: 从设置了过期时间的key中，删除最久未使用的key allkeys-lru: 从所有key中，删除最近最少使用的key volatile-lfu: 从设置了过期时间的key中，删除使用频率最少的key allkeys-lfu: 从所有key中，删除使用频率最少的key volatile-random: 从设置了过期时间的key中，随机删除 allkeys-random: 从所有key中，随机删除 volatile-ttl: 从设置了过期时间的key中，删除马上要过期的key noeviction: （Redis的默认淘汰策略）不删除，在写操作时直接报错 当mem_used达到maxmemory时，所有的写请求都会调用 src/evict.c 的 freeMemoryIfNeeded(void) 方法，执行对应的策略尝试释放一些内存。 近似的 LRU 算法Redis 中的LRU并不是真正的LRU（Least Recently Used），为了避免LRU算法中hash表的内存消耗，Redis 给每个key都增加了一个额外的小字段（lru）：一个24bit长度的，记录着最后一次被访问的时间戳”。 在执行 Redis LRU 算法时，从待删除的key空间中随机采样maxmemory-samples（默认是5）个key，淘汰最久未访问过的那个key（redis作者测试的结果是当maxmemory-samples = 10时，已经非常接近全量LRU的精准度了）。 123#define LRU_BITS 24#define LRU_CLOCK_MAX ((1&lt;&lt;LRU_BITS)-1) /* Max value of obj-&gt;lru */#define LRU_CLOCK_RESOLUTION 1000 /* LRU clock resolution in ms */ 注意：LRU 时间戳的默认精度是 1000ms，24bit 可以表示的最长时间大约是194天。 如果一个 key 超过194天没有被访问，在计算空闲时间的时候，就会少算194天，造成一定的误差。 为了避免 fork 子进程后额外的内存消耗，当进行 bgsave 或 aof rewrite 时，lru访问时间是不更新的。 123456789101112131415161718192021222324252627/* 如果对象本身存的lru访问时间 比系统的当前lru clock还要大 * 那么在计算当前对象的空闲时间的时候，就不能简单的lruclock - o-&gt;lru相减了，需要额外计算*/unsigned long long estimateObjectIdleTime(robj *o) &#123; unsigned long long lruclock = LRU_CLOCK(); if (lruclock &gt;= o-&gt;lru) &#123; return (lruclock - o-&gt;lru) * LRU_CLOCK_RESOLUTION; &#125; else &#123; return (lruclock + (LRU_CLOCK_MAX - o-&gt;lru)) * LRU_CLOCK_RESOLUTION; &#125;&#125;/* 如果当前 lru clock 的分辨率低于系统刷新频率，则使用系统的时间戳(server.lruclock) * 否则调用系统的mstime()函数重新计算*/unsigned int LRU_CLOCK(void) &#123; unsigned int lruclock; if (1000/server.hz &lt;= LRU_CLOCK_RESOLUTION) &#123; atomicGet(server.lruclock,lruclock); &#125; else &#123; lruclock = getLRUClock(); &#125; return lruclock;&#125;/* 当计算出的当前clock超出LRU_CLOCK_MAX时，就会再次从0开始数 * server.lruclock也是通过此方法来获取的当前时间*/unsigned int getLRUClock(void) &#123; return (mstime()/LRU_CLOCK_RESOLUTION) &amp; LRU_CLOCK_MAX;&#125; 在 Redis 3.0 之后，又增加了一个eviction pool的结构，eviction pool是一个数组，保存了之前随机选取的key及它们的idle时间，数组里面的key按idle时间升序排序，当内存满了需要淘汰数据时，会调用dictGetSomeKeys选取指定的数目的key，然后更新到eviction pool里面，如果新选取的key的idle时间比eviction pool里面idle时间最小的key还要小，那么就不会把它插入到eviction pool里面。这样就保证了对于某些历史选取的key的idle时间相对来说比较久，但是本次淘汰并没有被选中，因为出现了idle时间更久的key，那么在使用eviction pool的情况下，这种idle时间比较久的key淘汰概率增大了，因为它在eviction pool里面被保存下来，参与下轮淘汰，这个思路和访问局部性原理是契合的。 因此在只维护一个eviction pool带来的少量开销情况下，对算法效率的提升是比较明显的，效率的提升带来的是访问命中率的提升。 Redis 的 LFULFU（Least Frequently Used）是在Redis4.0后出现的，LRU的最近最少使用实际上并不精确，考虑下面的情况，如果在|处删除，那么A距离的时间最久，但实际上A的使用频率要比B频繁，所以合理的淘汰策略应该是淘汰B。LFU就是为应对这种情况而生的。 12A~~A~~A~~A~~A~~A~~A~~A~~A~~A~~~|B~~~~~B~~~~~B~~~~~B~~~~~~~~~~~B| 在 Redis 中每个对象都有24 bits空间来记录LRU/LFU信息： 123456789typedef struct redisObject &#123; unsigned type:4; unsigned encoding:4; unsigned lru:LRU_BITS; /* LRU time (relative to global lru_clock) or * LFU data (高16位用来记录访问时间（单位为分钟）， * 低8位用来记录访问频率，简称counter). */ int refcount; void *ptr;&#125; robj; counter计数器对数因子和衰减时间由于 LFU 的基数器只有8bit，所能记录的最大数字为255。因此counter并不是线性增长的，而是用基于概率的对数计数器来实现。counter的增长速度有lfu-log-factor参数控制。但最终counter总会收敛于255，为了解决这个问题，redis还提供了lfu-decay-time衰减因子（单位为分钟），如果一个key长时间未被访问，counter就要减少，减少的幅度由衰减因子来控制。 12345678910111213141516171819202122232425262728293031323334353637383940414243void updateLFU(robj *val) &#123; unsigned long counter = LFUDecrAndReturn(val); counter = LFULogIncr(counter); val-&gt;lru = (LFUGetTimeInMinutes()&lt;&lt;8) | counter;&#125;/* 高16位，当server.unixtime超出2^16时，再次从0开始计数 */unsigned long LFUGetTimeInMinutes(void) &#123; return (server.unixtime/60) &amp; 65535;&#125;/* redis的key的空闲时间(分钟) */unsigned long LFUTimeElapsed(unsigned long ldt) &#123; unsigned long now = LFUGetTimeInMinutes(); if (now &gt;= ldt) return now-ldt; return 65535-ldt+now;&#125;/* 在key被访问时，在counter++之前，要根据lfu_decay_time对counter做适当的修减： num_periods = 空闲时间/lfu_decay_time &gt; 0 counter = (num_periods &gt; counter) ? 0 : counter - num_periods; */unsigned long LFUDecrAndReturn(robj *o) &#123; unsigned long ldt = o-&gt;lru &gt;&gt; 8; unsigned long counter = o-&gt;lru &amp; 255; unsigned long num_periods = server.lfu_decay_time ? LFUTimeElapsed(ldt) / server.lfu_decay_time : 0; if (num_periods) counter = (num_periods &gt; counter) ? 0 : counter - num_periods; return counter;&#125;/* 基于概率的对数计数器： 1. 提取0到1之间的随机数R。 2. 计算counter++的概率 P = 1/(old_value*lfu_log_factor+1). 3. 如果R &lt; P，则counter++ */#define LFU_INIT_VAL 5 //counter的初始值为5，以便给新对象一个机会去积累点击率uint8_t LFULogIncr(uint8_t counter) &#123; if (counter == 255) return 255; double r = (double)rand()/RAND_MAX; double baseval = counter - LFU_INIT_VAL; if (baseval &lt; 0) baseval = 0; double p = 1.0/(baseval*server.lfu_log_factor+1); if (r &lt; p) counter++; return counter;&#125; 计数器对数因子lfu-log-factor默认是10，有redis.conf中的这张表可知，默认情况下，当访问次数达到100万时，counter才会增长到255。 计数器衰减时间lfu-decay-time是一个被衰减的key计数器所必须经过的时间（单位为分钟），默认是1。如果lfu-decay-time = 0，则代表每次在更新counter时，它都要被衰减。 12345678910111213# redis-benchmark -n 1000000 incr foo# redis-cli object freq foo# +--------+------------+------------+------------+------------+------------+# | factor | 100 hits | 1000 hits | 100K hits | 1M hits | 10M hits |# +--------+------------+------------+------------+------------+------------+# | 0 | 104 | 255 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 1 | 18 | 49 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 10 | 10 | 18 | 142 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 100 | 8 | 11 | 49 | 143 | 255 |# +--------+------------+------------+------------+------------+------------+ TTL跟近似的LRU一样，并不是去所有key中寿命最短的那个，而是每次随机取maxmemory-samples个key，删除这里面最即将过期的key。同样在Redis 3.0 之后由于引入了eviction pool数组，使内存淘汰key的策略更公平。 参考资料： Redis作为LRU Cache的实现 Redis近似LRU算法优化 Redis · 引擎特性 · 基于 LFU 的热点 key 发现机制]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GeoHash 算法]]></title>
    <url>%2Flearning%2FGeoHash%2F</url>
    <content type="text"><![CDATA[计算「附近的人」，当数据量并发量很小的时候，我们可以直接在数据库中拿到所有点的经纬度信息，然后利用勾股定理依次计算两点间距离，排序。但是当你的数据库里有几百万个点的时候，为了提高系统性能，我们可以先以一个点为中心画一个圆，然后计算这个圆中的所有点的距离。但是MySQL的性能毕竟有限，如果并发量也上来呢？恐怕现在还没有那个系统有这么强悍的计算量，因此GeoHash应运而生。 简单来说，GeoHash可以理解为另一种地址编码方式，它将二维空间的经纬度数据编码成一个字符串，字符串的二进制码值越接近，也说明两个点的实际距离约接近（存在一定的误差）。 注意：GeoHash值表示的是一个矩形区域，hash值越长，矩形的范围越小。例如wx4g0ec1，编码wx4g0e表示的范围就比wx4g0ec1更大。所以GeoHash可以用来快速圈定给定坐标的附近坐标。 此外（题外话）使用GeoHash来表示位置信息，也有助于隐私保护。在线根据GeoHash转换经纬度：http://geohash.org/{GeoHash} 算法实现将经纬度转换为二进制比如（39.923201, 116.390705）这个点，纬度的范围是（-90，90），其中间值为0。对于纬度39.923201，在区间（0，90）中，因此得到一个1；（0，90）区间的中间值为45度，纬度39.923201小于45，因此得到一个0，依次计算下去，即可得到纬度的二进制表示，如下表： 最后得到纬度的二进制表示为：10111000110001111001 同理可以得到经度116.390705的二进制表示为：11010010110001000100 合并经度、纬度的二进制将经度、纬度二进制按照奇偶位合并。 111100 11101 00100 01111 00000 01101 01011 00001 按照Base32进行编码将上一步合并后的二进制数转换为十进制，然后生成对应的Base32码（0-9,a-z 去掉 a,i,l,o 四个字母）。同理，将编码转换成经纬度的解码算法与之相反，具体不再赘述。 上述合并后的二进制编码后的结果为： 1wx4g0ec1 由此可见编码越长，表示的范围越小，位置也越精确。下表摘自维基百科： 可以看出，当geohash base32编码长度为8时，精度在19米左右，而当编码长度为9时，精度在2米左右，编码长度需要根据数据情况进行选择。 算法原理 为什么经纬度两串编码是交叉组合成一串编码的？ 这就要从GeoHash算法编码原理的起源说起了，简称“二刀法”：将空间划分为四块，编码的顺序分别是左下角00，左上角01，右下脚10，右上角11，也就是类似于Z的曲线，当我们递归的将各个块分解成更小的子块时，编码的顺序是自相似的，每一个子块也形成Z曲线，这种类型的曲线被称为Peano空间填充曲线。 但是Peano空间填充曲线最大的缺点就是突变性，有些编码相邻但距离却相差很远，比如0111与1000，编码是相邻的，但距离相差很大。 除Peano空间填充曲线外，还有很多空间填充曲线，如图所示，其中效果公认较好是Hilbert空间填充曲线，相较于Peano曲线而言，Hilbert曲线没有较大的突变。为什么GeoHash不选择Hilbert空间填充曲线呢？可能是Peano曲线思路以及计算上比较简单吧，事实上，Peano曲线就是一种四叉树线性编码方式。 使用问题Geo 的数据使用单独的 Redis 实例部署，不使用集群环境。因为单个key的数据量有可能很大，不利于集群的迁移。为了降低单个zset集合的大小，可以将数据按国家拆分、按省拆分、按市拆分。(每个省市拆分的时候，记得对边缘点进行一些冗余存储)。 矩形边缘点问题将其相邻的8个区域的点也都加进来，一起算距离。然后排序。 曲线突变问题Peano空间填充曲线，这种曲线会产生突变，造成了编码虽然相似但距离可能相差很大的问题，因此在查询附近餐馆时候，首先筛选GeoHash编码相似的POI点，然后再进行实际距离计算。 参考资料： GeoHash核心原理解析]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 都有哪些数据结构？]]></title>
    <url>%2Freading%2Fredis_datastruct%2F</url>
    <content type="text"><![CDATA[字符串String，字段Hash，列表List，集合Set，有序集合ZSet。 位图、HyperLogLog、GeoHash、PubSub、Stream Redis Module：RedisBloom、RedisSearch、Redis-Cell 位图用于统计用户进1年的签到情况。它的本质还是String，只是Redis天然支持位操作。（直接将byte数组看成「位数组」即可） setbit key offset value getbit key offset bitcount key [start] [end] #统计两个offset之间1的个数 bittop op destkey [key…] #对两个位图进行and、or、not、xor操作，将结果保存在destkey中 bitfield key get/set/incrby HyperLogLog用于统计页面访问UV、直播在线人数、微博点赞数、微博词条搜索次数，等这些对统计精度要求不是100%，且不需要返回统计集合内所有的元素的场景。原理：N = 2^{k_{max}}，利用每个桶内的k_{max}的调和平均值估计集合的整体基数。 pfadd key element pfcount key pfmerge [key…] 详情见HyperLogLog Counting算法 GeoHashRedis 在 3.2 版本以后增加了地理位置 GEO 模块，意味着我们可以使用 Redis 来实现摩拜单车「附近的 Mobike」、饿了么「附近的餐馆」这样的功能了。业界比较通用的地理位置距离排序算法是 GeoHash 算法，Redis 也使用 GeoHash 算法。它将二维的经纬度数据映射成一维的整数，这样所有的元素都将在挂载到一条线上，距离靠近的二维坐标映射到一维后的点之间距离也会很接近。在使用 Redis 进行 Geo 查询时，它的内部结构实际上只是一个 zset(skiplist)。通过 zset 的 score 排序就可以得到坐标附近的其它元素，通过将 score 还原成坐标值就可以得到元素的原始坐标。 geoadd key (longitude) (latitude) (address) geodist key (addressA) (addressB) (km) geopos key address #获取元素的经纬度信息 geohash key address #获取元素的hash值 georadiusbymember key address 20 km [withcoord] [withdist] [withhash] count 3 asc # 查询附近20km内最近的3个元素，正序并显示经纬度、距离、hash信息 PubSub（消息多播）可以多个客户端订阅同一个消息主题，实现消息多播。但是此模式的缺点就是一旦Redis停机重启，PubSub 的消息是不会持久化的，毕竟 Redis 宕机就相当于一个消费者都没有，所有的消息直接被丢弃。 Redis5.0 新增的 Stream 数据结构，这个功能给 Redis 带来了持久化消息队列，从此 PubSub 可以消失了 StreamStream 的消费模型借鉴了 Kafka 的消费分组的概念，它弥补了 Redis Pub/Sub 不能持久化消息的缺陷。但是它又不同于 kafka，Kafka 的消息可以分 partition，而 Stream 不行。 如果非要分 parition 的话，得在客户端做，提供不同的 Stream 名称，对消息进行 hash 取模来选择往哪个 Stream 里塞。 RedisBloom（布隆过滤器）它说有，不一定有。它说没有，就一定没有。 因为布隆过滤器的实现原理是利用一个长度为l的位数组，没bf.add一个元素进来，会经过k个hash函数，得到k个不同的位置，然后将这k个位置的数字置位1。当调用bf.exists时，直接去这个key得到的k个hash位置判断是否都为1，有一个位置为0，这判为不存在。但是即使都为0，这个key也不一定存在，因为这k个hash位也有可能是由其他若干个key置位1的。 bf.add key value # bf.madd key [value…] bf.exists key value # bf.mexists key [value…] 使用场景垃圾邮件的过滤、今日头条推荐文章去重、判断一个商户是不是我这个平台的（防止缓存穿透）、爬虫系统url去重（当爬取网页url的数量达到几千万几亿的级别时）等等这种数据量很大，判断key是否存在（去重、过滤），且允许一定几率误判的业务场景。 调优Redis 官方提供的布隆过滤器到了 Redis 4.0 提供了插件功能之后才正式登场。在执行bf.add(key)之前会使用bf.reserve执行显示创建BoolmFIlter，两个个关键参数： f（error_rate(0.01)）数字越小，占用空间越大。 n（initial_size(100)）当key的实际数量超出这个数值时，误判率会上升。 这两个会直接影响布隆过滤器的数组长度(l)、和hash函数的最佳数量(k)简单的计算公式如下： k ≈ 0.7 * \frac{l}{n} f = 0.6185^\frac{l}{n} 公式的证明：数学学渣慎点线上计算空间占用：https://krisives.github.io/bloom-calculator/ 误判为什么会上升？ RedisSearch（全文搜索引擎）Redis-Cell]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim常用命令]]></title>
    <url>%2Ftool%2Fvim_skills%2F</url>
    <content type="text"><![CDATA[Vimer：https://vimjc.com]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IntelliJ IDEA 实用插件推荐]]></title>
    <url>%2Ftool%2Fidea_plugin%2F</url>
    <content type="text"><![CDATA[参考资料：https://mp.weixin.qq.com/s/LtEcayFGcc993gHfS6klAQ Lombok在编译期自动生成get、set 等方法 GenerateAllSetter一键调用一个对象的所有的set方法 Alibaba Java Coding Guidelines阿里巴巴出品的java代码规范插件，可以扫描整个项目找到不规范的地方 并且大部分可以自动修复 。虽说检测功能没有findbugs强大，但是可以自动修复，阿里巴巴Java编码指南插件支持。 VisualVM Launcher运行java程序的时候启动visualvm，方便查看jvm的情况 比如堆内存大小的分配某个对象占用了多大的内存，jvm调优必备工具 客户端：https://visualvm.github.io/download.html安装完客服端，需在Preferences -&gt; Other Settings -&gt; VisualVM Launcher 绑定VisualVM excutable，（客户端start文件） Maven Helper一键查看maven依赖，查看冲突的依赖，一键进行exclude依赖]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Mac</tag>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HyperLogLog Counting 算法]]></title>
    <url>%2Flearning%2FHyperLogLog%2F</url>
    <content type="text"><![CDATA[知其然必知其所以然，最近在看redis的义总问了我一个问题：一个网站的uv是怎么实现的？（当访问量非常大的时候，uv&gt;1000,000,000的时候，还能用redis吗？）我最近正好搭了这个博客，pv和uv都是直接用的不蒜子这个插件，前两天也正好在想他这个uv是怎么实现的？ 于是义总带我走进了HyperLogLog的世界：（以下简称HLL算法） 参考资料： 神奇的HyperLogLog算法 《Redis 深度历险：核心原理与应用实践》 HLL算法演示：http://content.research.neustar.biz/blog/hll.html HLL算法的使用场景HyperLogLog Counting是一种基数计数方法，它的优点：速度极快（常数时间），占用空间极小。缺点：只适用于统计页面UV(unique visitor)、微博的点赞数这种对精确度要求不是很高，且不需要返回集合内所有元素的场景。 已知1亿bit数据，占用空间100000000/8/1024/1024≈12MB。 假设我们统计的是用户ip地址，每个ip地址占用空间32bit。 几种基数计数方法，统计1个页面，数据量达到1亿条ip时，所占用的空间及时间复杂度对比： B树 Bitmap redis的HyperLogLog算法 占用空间大小 32x12 ≈ 384MB 12MB 12KB 时间复杂度 O(logN) O(N) O(1) HyperLogLog另一个特性想统计今天和昨天两天合起来的uv，HyperLogLog也可以直接取两个天的ip取并集，返回近两天的uv。而其他算法只能重新统计。 HLL算法原理在基数计数方面，HyperLogLog之所以这么牛X，这么神秘，完全是因为它站在了概率统计这个巨人的肩膀上，根据伯努利实验总结出公式如下： N = 2^{K_{max}}N：整个数据集合里面数据的数量（基数）K：数据集每进来一个元素，对这个元素进行一次二进制hash，k表示hash值第一个1的位置。Kmax：所有这些K中，最大的那个值。（它一定小于等于N） （伯努利实验）证明：抛硬币，正面朝上记为1，反面朝上记为0，连续抛X次，第一次出现正面时的投掷次数记为k。这个抛硬币的过程称为伯努利过程。对于n次伯努利过程，我们可以得到n个首次出现正面的投掷次数k_1, k_2……k_n，其中最大值记为k_{max}，那么可以得到下面两个结论： n次伯努利过程的首次出现正面的投掷次数都不大于k_{max} n次伯努利过程，至少有一次的首次出现正面的投掷次数等于k_{max} 对于结论1，用数学公式可以表示为：P_n(X ≤ k_{max}) = (1 - 1/2^{k_{max}})^n，当n ≫ 2^{k_{max}}时，P_n(X ≤ k_{max}) ≈ 0。但是P_n(X ≤ k_{max}) = 1，所以n不可能远大于2^{k_{max}}。对于结论2，用数学公式可以表示为：P_n(X ≥ k_{max}) = 1 - (1 - 1/2^{k_{max}-1})^n，当n ≪ 2^{k_{max}}时，P_n(X ≥ k_{max}) ≈ 0。但是P_n(X ≥ k_{max}) = 1，所以n也不可能远小于2^{k_{max}}。 因此我们可以得出结论，n ≈ 2^{k_{max}}。 HLL算法讲解分桶平均很显然，每次根据抛硬币来推测：我这种情况抛多少次能抛出来。这种“预估方法”误差有点大。因此 HLL算法并不适用于当数据量非常小的时候，统计集合的基数。 只有当数据量上来的时候，做这种统计估计才有意义。同时HLL算法为了节省空间，引入了分桶平均的概念。基本原理是：将统计数据划分为m个桶（将元素的二进制hash % m），分别统计各个桶的k_{max}值，求调和平均数（所有数的倒数的平均数，可以消除掉极大值和极小值对平均数结果带来的误差），进而得出整个集合的基数预估值 N。 Java代码实现：HyperLogLogCounting 偏差修正上述经过分桶平均后的估计量看似已经很不错了，不过通过数学分析可以知道这并不是基数N的无偏估计。这部分的具体数学分析在“Loglog Counting of Large Cardinalities”中。 因此需要修正成无偏估计。方法： 增大桶的数量（增大数组长度）桶越多，每个统计数组越长，误差越小，但存储成本也就越大。 分阶段修正。 Redis中HyperLogLog的实现Redis应用HyperLogLog的三个命令：PFADD、PFCOUNT、PFMERGE Redis 的 HyperLogLog 实现中使用了 16384 个桶，也就是 2^14。每次进来一个元素，对这个元素进行一次二进制hash的长度为64bit，因此每个桶需要用6bit的空间来装这个桶里面的Kmax值。所以总共占用内存就是 2^14 * 6 / 8 = 12k 字节。因为最大可以K值为63，所以redis理论上可以统计的集合的基数的最大值为 2^63。 同时Redis 对 HyperLogLog 的存储进行了优化，在计数比较小时，它的存储空间采用稀疏矩阵存储，空间占用很小，仅仅在计数慢慢变大，稀疏矩阵占用空间渐渐超过了阈值时才会一次性转变成稠密矩阵，才会占用 12k 的空间。 附两个数学公式的白话解释 结论1【n次伯努利过程的首次出现正面的投掷次数都不大于k_{max}】数学公式 P_n(X ≤ k_{max}) = (1 - 1/2^{k_{max}})^n 的白话解释： 已知1次抛硬币正面朝上的概率为1/2，反面朝上的概率为(1 - 1/2) 1次伯努利过程，第k_{max}次还没出现正面（前k_{max}次都是反面）的概率为：(1 - 1/2)^{k_{max}} = (1 / 2)^{k_{max}} = 1 / 2^{k_{max}} 1次伯努利过程，投掷不大于k_{max}次就出现正面的概率为：1 - 1 / 2^{k_{max}} n次伯努利过程，投掷不大于k_{max}次就出现正面的概率为：(1 - 1 / 2^{k_{max}})^n 结论2【n次伯努利过程，至少有一次的首次出现正面的投掷次数等于k_{max}】公式P_n(X ≥ k_{max}) = 1 - (1 - 1/2^{k_{max}})^n 的白话解释： 已知1次伯努利过程，投掷不大于k_{max}次就出现正面的概率为：1 - 1 / 2^{k_{max}} n次伯努利过程，P (至少有一次投掷次数为k_{max})= 1 - P(每次投掷次数都小于k_{max})= 1 - P(每次投掷次数都 ≤ k_{max} - 1)= 1 - P(每次投掷次数都不大于k_{max} - 1)= 1 - (1 - 1/2^{k_{max} - 1})^n]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于 Redis]]></title>
    <url>%2Freading%2Fredis%2F</url>
    <content type="text"><![CDATA[Remote Dictionary Service参考资料： 《Redis 深度历险：核心原理与应用实践》 Redis 源码（版本：5.0.5） Redis 都有哪些数据结构？Redis 过期key删除策略和内存淘汰策略关于Redis 高可用主从同步SentinelCodis vs Cluster限流 - 时间窗口分布式锁Resis漏洞线程 IO 模型Info指令]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[莫里斯遍历]]></title>
    <url>%2Flearning%2Fmorris%2F</url>
    <content type="text"><![CDATA[遍历二叉树的神级方法：时间复杂度O(h)，空间复杂度O(2)（只用两个辅助变量）]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL - 锁]]></title>
    <url>%2Flearning%2Fmysql_lock%2F</url>
    <content type="text"><![CDATA[参考：https://www.cnblogs.com/zhoujinyi/p/3435982.htmlhttps://tech.meituan.com/2014/08/20/innodb-lock.html MVCCNext-Key Lock]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL - 基本概念]]></title>
    <url>%2Flearning%2Fmysql_basis%2F</url>
    <content type="text"><![CDATA[关于存储引擎InnoDB存储引擎的特点：支持外键、行级锁、每个表单单独位于一个表空间。 MyISAM和InnoDB的比较： 项目 MyISAM InnoDB 索引结构 B+ 树 B- 树 事务 ❌ ✔️ 锁粒度 表级锁 表级锁&amp;行级锁(锁的是索引对应的那一行) 外键约束 ❌ ✔️ 表空间大小 相对小 相对大 全文索引 ✔️ ❌ 关注点 性能(select) 事务(work) 数据文件 每张MyISAM表存放在三个文件中：元数据：*.frm数据文件：*.MYD索引文件：*.MYI (默认)共享表空间 其他 支持全文索引、压缩索引 不支持 关于索引 项目 MyISAM InnoDB 1 升序顺序存储数据，叶子节点保持对应数据行的地址 主键节点同时保存数据行，其他辅助索引保存的是主键索引的值 2 键值分离，索引载入内存，数据缓存依赖操作系统 键值一起保存，索引与数据一起载入InnoDB缓冲池 索引的基数值 精确的 估计的 事物隔离级别参考：MySQL 四种事务隔离级的说明、Innodb中的事务隔离级别和锁的关系 隔离级别 脏读（Dirty Read） 不可重复读（NonRepeatable Read） 幻读（Phantom Read） 解释 未提交读（Read uncommitted） ✔️ ✔️ ✔️ 允许脏读，也就是可能读取到其他会话中未提交事务修改的数据 已提交读（Read committed） ❌ ✔️ ✔️ 只能读取到已经提交的数据。Oracle等多数数据库默认级别 (不重复读) 可重复读（Repeatable read） ❌ ❌ ✔️ 可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB默认级别。 可串行化（Serializable ） ❌ ❌ ❌ 完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞 脏读（Dirty Read）脏读就是指当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。 不可重复读（NonRepeatable Read）/ 已提交读（Read committed）是指在一个事务内，多次读同一数据。在这个事务还没有结束时，另外一个事务也访问该同一数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改，那么第一个事务两次读到的的数据可能是不一样的。这样就发生了在一个事务内两次读到的数据是不一样的，因此称为是不可重复读。 可重复读（Repeatable read）本身，可重复读和提交读是矛盾的。在同一个事务里，如果保证了可重复读，就会看不到其他事务的提交，违背了提交读；如果保证了提交读，就会导致前后两次读到的结果不一致，违背了可重复读。 可以这么讲，InnoDB提供了这样的机制，在默认的可重复读的隔离级别里，可以使用加锁读去查询最新的数据。 幻读（Phantom Read）第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样。 当隔离级别是可重复读，且禁用innodb_locks_unsafe_for_binlog的情况下，在搜索和扫描index的时候使用的next-key locks可以避免幻读。 附：CentOS安装大法 yum –y install mysql yum –y install mysql -server service mysqld start //启动 /usr/bin/mysqladmin –u root password ‘root’ //设置密码 mysql –u root –p //进入 use mysql; select host,user,password from user; delete from user where password=’’; 12/usr/bin/mysqladmin -u root password &apos;new-password&apos;/usr/bin/mysqladmin -u root -h vultr.guest password &apos;new-password&apos;]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Valine 初始化头像]]></title>
    <url>%2Ftrying%2Fd6b91886-valine_avatar%2F</url>
    <content type="text"><![CDATA[背景 添加了Valine评论插件后，发现评论者的头像是在_config.yml里面设置一种样式。 如果想改头像只能去Gravatar自行注册账号修改头像。（https://valine.js.org/avatar.html） 默认头像如果评论多了的时候，好难看： 目标本地初始化头像，使相同昵称、相同邮箱的评论者，展示相同的头像。且 兼容远程头像（用户自己去Gravatar设置过头像的，展示用户自己的头像） 行动（上述目标，其实是在经过2个小时采坑填坑之后的最终结果）大致说下采坑过程，与解题思路。感谢舍友的大力支持，不然我这前端小白2天都搞不定。/捂脸 先Google，发现个类似问题：Hexo使用Valine评论系统不显示自定义头像的解决方案 让我知道了Valine的头像是有一组值的 ds: [&quot;mp&quot;, &quot;identicon&quot;, &quot;monsterid&quot;, &quot;wavatar&quot;, &quot;robohash&quot;, &quot;retro&quot;, &quot;&quot;], 这就让我有了一个大胆的想法：就用这几个官方头像模板初始化。 获取官方最新的Valine.min.js文件开始改造过程就不详细记录了，大致说下踩到的几个坑： 直接随机显示ds数组里的这几个头像，带来的问题：相同昵称（同一个人）头像可能不一样 拿到的是Valine.min.js压缩过的js，美化后（格式化），无法解析。必须直接去压缩文件中修改js代码 换个思路，将ds数组随机的值，与评论者的昵称做匹配。 看了下Valine的源码：https://github.com/xCss/Valine/blob/master/src/index.js 原来这个头像链接的组成是这样的： `&lt;img class=&quot;vimg&quot; src=&quot;${_avatarSetting[&#39;cdn&#39;]+md5(rt.get(&#39;mail&#39;))+_avatarSetting[&#39;params&#39;]}&quot;&gt;`; src=&quot;https://gravatar.loli.net/avatar/d41d8cd98f00b204e9800998ecf8427e?d=mp&amp;v=1.3.9&quot; 【cdn url】 + 【mail的md5（用于查询用户自定义的头像）】 + 【config.yml中配置的默认头像】 + 【version】 那我只需要把第三部分一改：根据nick的md5值，随机一个0-5的数字，返回ds数组中对应的值。 _nick = a(e.get(&quot;nick&quot;)), _res = _nick.match(/(\d)/)[1], _seed = _res ? Math.floor(_res / 2) : &#39;5&#39;, o = m.hide ? &quot;&quot; : &#39;&lt;img class=&quot;vimg&quot; src=&quot;&#39; + (m.cdn + a(e.get(&quot;mail&quot;)) +&quot;?d=&quot;+(m.ds[_seed] || &quot;mp&quot;) + &#39;&amp;v&#39; + m.params.split(&quot;&amp;v&quot;)[1]) + &#39;&quot;&gt;&#39;, 大功告成：My Valine.min.js文件 改造后]]></content>
      <categories>
        <category>搞点事情</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM 实战]]></title>
    <url>%2Flearning%2Fjvm_action%2F</url>
    <content type="text"><![CDATA[常用的Jvm启动参数 -Xmx512m：设置JVM最大可用内存为512M。 -Xms512m：设置JVM初始内存为512m。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。 -Xmn200m：设置年轻代大小为200M。整个堆大小=年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8（young占30%左右） -verbose:gc：开启gc日志 -Xloggc:gcc.log：将日志输出到文件xx(默认位置为桌面) -XX:+PrintGCDetails：打印GC详情 -XX:+PrintGCDateStamps：打印GC时间戳 参考：GC性能优化美团 - 从实际案例聊聊Java应用的GC优化 FGC实战以JxlTest为例，通过gc日志，fix FGC问题。参考阿飞的博客-FGC实战 阅读一下GC日志]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 虚拟机 - 概念]]></title>
    <url>%2Freading%2F6f8dd084-JVM%2F</url>
    <content type="text"><![CDATA[书单列表： 《深入理解Java虚拟机（第2版）》 《实战Java虚拟机》 目录： Java虚拟机的基本结构 常用的垃圾回收算法 引用计数法 标记清除法 复制算法 标记压缩法 分代算法 分区算法 垃圾回收器 串行回收器 并行回收器 CMS回收器 G1回收器 Class装载系统 类装载步骤 双亲委托模式 字节码执行 Class文件结构 JIT Java虚拟机的基本结构 Java栈：由栈帧组成，一个函数对应一个栈帧(函数被调用—&gt;入栈)，[栈顶—-当前正在运行的函数]，[弹栈—&gt;①函数正常return;②Exception] 方法区： JDK1.6、1.7 ==&gt; 又叫永久区(Perm) JDK1.8 ==&gt; 叫元数据区(Metaspace)，(默认 可以吃掉所有系统可用内存) 常用的垃圾回收算法“垃圾”：内存中，不会再被使用的对象。 1. 引用计数法==&gt;缺点：①无法处理循环引用；②&#39;+&#39;、&#39;-&#39;计数器浪费性能 2. 标记清除法 标记：标记从 根节点 开始的所有可达对象 清除：清除所有未被标记的对象 ==&gt;缺点：回收后的空间不是连续的，在给大对象分配堆空间时 效率低。 3. 复制算法(适用于 存活对象少，垃圾对象比较多的新生代)例：将内存(Java堆)平均分为两块A、B，每次清理只需要将A中存活的对象复制到B中，然后清空A。 从而解决了 回收后空间不连续的问题 4. 标记压缩清除法(适用于老年代) 标记 从 根节点 开始的所有可达对象 压缩到内存的一端 清除边界外的所有垃圾 5. 分代算法即 新生代用“**改良的复制算法**”，老年代用“**标记压缩清除法**” ![改良的复制算法](http://tb.nsfocus.co/image/6f8dd084-3.png) - 栈上分配：基于逃逸分析技术，将线程私有的*小对象*打散分配到Java栈上。（函数调用结束后，会自行销毁，不需要垃圾回收器介入从而提高了性能） - 逃逸分析：对象的作用域未逃出函数体(-server模式下 默认开启) - TLAB（Thread-local allocation buffer）：线程本地分配缓存(是一个线程专用的内存分配)。（实质：线程专属区间 ⊂ Eden区间） - 在老年代引入**卡表**(比特位集合，每一位表示老年代的4k空间)，（1：有被新生代引用的对象，0：无,,,,,,）以应对新生代的高频率GC，每次只需扫描卡表中为1的空间即可。 6. 分区算法将整个堆划分成连续的不同的小区间，每个小区间独立使用，独立回收。 从而能够更好地控制一次GC产生的停顿时间 垃圾回收器 回收器 串行/并行 算法 特点 新生代串行回收器 单线程 复制算法 实现简单 逻辑处理高效 老年代串行回收器 单线程 标记压缩清除 (堆空间较大时)停顿时间长 新生代ParNew回收器 多线程 复制算法 性能要看CPU的并发能力 新生代ParallelGC回收器 多线程 复制算法 [MaxGCPauseMillis][GCTimeRatio] 关注吞吐量 老年代ParallelGC回收器 多线程 标记压缩清除 关注吞吐量 1. CMS回收器[关注系统停顿时间]（Concurrent Mark Sweep）并发标记清除 初始标记：【STW】 标记根对象 并发标记：标记所有可达对象 预清理：正式清理前的准备和检查(并发)，尝试控制一次停顿时间 重新标记：【STW】 修正并发标记 并发清理：(并发)回收垃圾对象 并发充值：重新初始化CMS数据结构和数据 2. G1回收器(Garbage First) 新生代GC：回收处理Eden和SurvivorA区。==&gt; 老年代的区域增多 并发标记周期：SATB(Snapshot-At-The-beginning)：在初始标记时为存活对象建立的快照 混合回收 正常的年轻代GC(改良的复制算法) 优先清理回收集中垃圾比例较高的区域 必要时的Full GC（回收时，内存不足的时候） 新生代GC时，survivor区和老年代区无法容纳幸存对象时 并发标记时，老年代被快速填充 混合GC时，发生空间不足 Class装载系统1. 类装载步骤 - 加载：解析类方法区内的数据结构 并创建实例 - 验证：class文件格式、语义检查、字节码验证、符号引用验证... - 准备：为类在Java堆中分配空间，设置初始值 - 解析：将 类、接口、字段、方法的符号引用转为直接引用 - 初始化：编译器自动生成并执行类的初始化方法```&lt;clinit&gt;```（方法```&lt;clinit&gt;```由类静态成员的赋值语句和static语句块合并产生） 2. 双亲委托模式(系统默认使用) 在类加载的时候， if（当前类已经被加载）{ return 当前可用类; } else { 请求其双亲加载器加载此类； if（success）{ return 可用的类； } else{ 自己加载； return 类； } } 字节码执行 本文为原创文章，包含脚本行为，会经常更新知识点以及修正一些错误，因此转载请保留原出处，方便溯源，避免陈旧错误知识的误导，同时有更好的阅读体验。本文地址：http://nibnait.com/6f8dd084-about-Java-Virtual-Machine/]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络发展史]]></title>
    <url>%2Freading%2Fc500696f-computer_networking_history%2F</url>
    <content type="text"><![CDATA[首先分享一段 《互联网 发展 历史》 的8分钟小视频 概念：将一个个独立的计算机连接起来，即组成了计算机网络。再结合这张图，不难看出 计算机的发展史，也就是计算机网络的发展史 计算机网络的基础知识在计算机网络发展的初期，各家计算机厂商都发明了一套自己的计算机通信协议，不同品牌的计算机无法进行通信，于是ISO▼制定了一个国际标准 OSI协议。 OSI七层参考模型▼： 搭建网络的主要设备 设备 别名 作用 网卡 NIC▼/网络适配器/LAN卡 使计算机连网 中继器(Repeater) 集线器(Hub)==多口中继器 在物理层，将减弱的信号放大并发送 网桥(Bridge) 2层交换机/以太网交换机/链路层交换机/交换集线器(具有网桥功能的Hub) 在数据链路层，校验并转发 数据帧 （将损坏的数据直接丢弃）（ 传送门：各种校验-CRC▼） 路由器(Router) 3层交换机 在网络层，通过IP协议根据IP地址对 数据报 进行转发 4~7层交换机 · 负载均衡· 防火墙(网关)· 带宽控制(优先处理一些及时性要求比较高的通信请求：语音通话)· 特殊应用访问加速、广域网加速器…. 网关(Gateway) · 协议的转换(表示层)与数据的转发(4~7层) 应用网关 代理服务器 · 是网关的一种· 可以从传输层到应用层对数据和访问进行各种控制和处理 传输方式的分类1. 面向有连接型与面向无连接型 面向有连接型 TCP 打电话建##立/断开连接的时候、 面向无连接型 UDP、IP 邮局寄包裹(不需要确认收件人的详细地址是否真实存在)、视频/语音通信的过程(没有必要保证每一帧都成功的传送到达目的地)、 2. 电路交换与分组交换 电路交换 主要用于：过去的电话网。 两台主机通信时 是独占整个电路的。同一电路的其他多台计算机只能等待正在通信的计算机收发信息完毕以后 才有机会使用这条电路。 分组交换 主要用于：现代计算机网络 处理过程：发送端计算机将数据分组发送给路由器，路由器收到这些分组数据以后，缓存到自己的缓冲区，然后再转发给目标计算机 限制：根据网络的拥堵情况，数据到达目标地址的时间有长有短。路由器的缓存饱和或溢出时，容易造成数据丢失。 3. 按接收端的数量分类 单播(Unicast) 1v1通信，打电话。 广播(Broadcast)：同一数据链路内的所有主机 电视频道、收音机频道。 多播(Muticast)：与特定组内的计算机通信 电话/视频会议 “1年1班的同学们，请起立！” 任播(Anycast)：解析特定组内的任意一台计算机 DNS根域名解析服务器(负载均衡) “1年1班的哪位同学上来领一下奖状？” “飞机上哪位乘客是医生？” 本文中提到的一些专有名词： ISO：国际标准化组织(International Organization for Standards) OSI：开放式通信系统互联参考模型(Open System Interconnection) NIC：网络接口卡(Network Interface Card) CRC：循环冗余校验(Cyclic Redundancy Check) WAN：广域网(Wide Area Netword) LAN：局域网(Local Area Network) Internet：网际网（WAN√、LAN√） The Internet：互联网]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络 - 进阶]]></title>
    <url>%2Freading%2Fb3dcfa99-computer_networking_advance%2F</url>
    <content type="text"><![CDATA[目录： 网络层 IP多播 IP隧道 IPv6的地址结构 路径MTU发现 ICMP报文消息类型详解 ICMPv6的邻居探索 1. 网络层IP多播： 附：既定已知的多播地址.png IP隧道 用处： MobileIP 多播包的转播（使用IP隧道，使路由器用单播的形式发包） IPv4网络中传送IPv6的包 √ IPv6网络中传送IPv4的包 √ 将数据链路的PPP包用IP包转发时 IPv6的地址结构 路径MTU发现 ICMP报文消息类型详解 ICMP目标不可达消息（类型3） 错误代码1（主机不可达）：路由表中没有该主机信息，或该主机(关机)没有连接到网络 错误代码2（协议不可达）(Protocol Unreachable) 错误代码3（端口不可达）(Port Unreachable) 错误代码4（Fragmentation Needed and Don’t Fragment was Set）：用于MTU探索 ICMP重定向消息（类型5） ICMP超时消息（类型11） 当TTL==0时，IP路由器就会给发送端主机返回一个 ICMP超时消息。 应用：tracerouter。 ICMP回送消息（类型0、8） 用于进行通信的主机或路由器之间，判断所发送的数据包是否已经成功到达对端的一种消息。 应用：ping命令。 ICMP路由探索消息（类型9、10） 主要用于发现与自己相连的网络中的路由器。当一台主机发出ICMP路由请求（Router Solicitaion，类型10）时，路由器则返回ICMP路由器通告消息（Router Advertisement，类型9）给主机。 ICMP地址掩码消息（类型17、18） 主要用于主机或路由器想要了解子网掩码的情况。可以向那些目标主机或路由器发送ICMP地址掩码请求消息（ICMP Address Mask Request，类型17），然后通过接收ICMP地址掩码应答消息（ICMP Address Mask Reply，类型18）获取子网掩码的信息。 ICMPv6的邻居探索 附：ICMPv6常用的报文消息类型 本文中提到的一些专有名词： IGMP：Internet Group Management Protocol MLD(Multicast Listener Discovery)：多播监听发现。 MSS：最大段长度 链路本地单播地址：同一链路内唯一的地址。 AS(Autonomous System)：自治系统 本文为原创文章，包含脚本行为，会经常更新知识点以及修正一些错误，因此转载请保留原出处，方便溯源，避免陈旧错误知识的误导，同时有更好的阅读体验。本文地址：http://nibnait.com/b3dcfa99-Computer-Networking-advanced/]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络 - 基础]]></title>
    <url>%2Freading%2F58316970-computer_networking_basis%2F</url>
    <content type="text"><![CDATA[书单列表： 《图解TCP/IP(第5版)》 《计算机网络：自顶向下方法(原书第6版)》 目录： 数据链路层 共享介质型网络 非共享介质型网络 以太网 VLAN PPP协议 网络层 IPv4编址 NAT技术 IPv6与IPv4数据报格式对比 IP报文的分片与重组 IP协议相关技术 ARP RARP ICMP ICMPv6 DHCP 路由选择算法 距离向量算法 - RIP、RIP2 链路状态算法 - OSPF BGP - 路径向量型协议 传输层 UDP - 用户数据报协议 TCP - 传输控制协议 其他传输层协议 应用层 远程登录 — TELNET、SSH 文件传输 — FTP 电子邮件 — MIME、SMTP、POP、IMAP WWW — HTTP 网络管理 — SNMP、MIB、RMON TCP/IP的诞生： 20世纪90年代，ISO展开了OSI这一国际标准协议的标准化进程。然而，OSI协议并没有得到普及，真正被广泛使用的是TCP/IP协议。究其原因，是因为TCP/IP的标准化过程是一个极其注重实用性的一个流程。 TCP/IP与OSI参考模型 数据链路层在LAN内，校验并转发 数据帧(根据MAC地址▼) 共享介质型网络 多个设备共享同一个载波信道进行发送和接收数据帧，半双工通信▼。 争用方式：CSMA/CD▼ 发前先听 边发边听 冲突即停 延时重发（指数退避等待） 令牌传递方式： 提高网络性能的两种方法：令牌释放(不等待接收方的数据到达确认就将令牌发送给下一站)、令牌追加(多个令牌同时循环) 非共享介质型网络 不共享介质。网络中的每个站直连交换机，由交换机负责转发数据帧。由于发送端和接收端并不共享通信介质，所以这种通信方式是全双工的。 以太网交换机(网桥)：转发表自学原理： 学习源地址 转发同网帧 丢弃异网帧 广播未知帧 环路检测技术： 生成树协议(STP▼)：通过检查网络的结构 以某一个网桥为构造树根，并对每个端口配置权重，来指定优先使用哪些端口以及发生问题时该使用哪些端口。 以太网——有线局域网技术历史：共享介质型网络—&gt;非共享介质型网络 以太网帧格式： VLAN(virtual LAN，虚拟局域网) 在链路层交换机(网桥)上，使用软件将一些端口逻辑地划分成的一个个网段。 可以跨越不同子网、不同类型网络(以太网、FDDI▼、ATM▼等)从而简化网络管理，减少设备投资。 交换机根据VLAN ID， 可以过滤多余的包，从而达到控制流量 减少了网络负载 并 提高了网络的安全性。 PPP(Point-to-Point Protocol): 拨号上网时，一个将IP数据报封装到串行链路的方法。 主要功能： - LCP(链路控制协议)：负责建立、(加密拨号登陆)配置和测试数据链路连接 - NCP(网络控制协议)：负责设置IP地址(IPCP)、连接上一层 网络层IP协议：尽力而为(面向无连接的) 一些特殊的IP地址： 网络号 主机号 10进制表示 源地址 目的地址 含义 0 0 0.0.0.0 ✔️ ❌ 本网络上的本主机 0 host-id ✔️ ❌ 本网络上的某台host-id任播 全1 全1 255.255.255.255 ❌ ✔️ 本网络上的广播,不能被路由器转发 net-id 全1 ❌ ✔️ 指定net-id上的所有主机广播 127 任何数(但不能都是0或1) 127.×××.×××.×××（默认：127.0.0.1=localhost） ✔️ ✔️ 本地回环测试地址 NAT▼技术：极大的缓解了IPv4地址不够用的囧境 在本地网络中使用私有地址，在连接互联网时转而使用全局IP地址的技术。 潜在问题：①无法从NAT外部向内部服务器建立连接；②转换表的生成与转换操作都会产生一定的开销。 解决地址短缺的最佳途径还是：普及IPv6。 IPv6 附：IPv4与IPv6数据报格式详解.png IP报文的分片与重组 IPv4:在路由器上分片，在接收端主机上重组。 IPv6:在发送端主机上分配，接收端主机重组。最小的MTU==1280字节 通过“路径MTU发现▼”，以确定分片时最大的MTU的大小。 主要利用一个ICMP不可达消息 将数据链路上MTU的值返回给发送机 ARP▼ 发送端主机根据IP地址，广播寻找其对应的MAC地址，（只适用于IPv4，IPv6中则可以用ICMPv6替代ARP发送邻居探索消息）实现链路内的IP通信。 附：ARP包格式.png ARP缓存表 通常发送端和接受端会把自己接收到的 带有IP地址和MAC地址的ARP响应包/ARP请求包中的信息缓存20分钟。以防止ARP包在网络中被大量广播，造成网络拥塞。 tips：虽然IP地址直到到达目标主机时都没有发生变化，但是数据链路的目标地址(MAC地址)却根据每个链路的不同而发生着变化。 MAC地址和IP地址缺一不可 在以太网上发送IP包时，“下次要经过哪个路由器发送数据报”中的“下一个路由器”就是其相应的MAC地址。 而如果没有IP地址，各个主机通信全靠广播MAC地址，那对于网桥来说将是一场灾难。 RARP 将ARP反过来，从MAC地址定位IP地址。用于将打印机服务器等小型嵌入式设备接入到网络时。 辅助IP的ICMP▼ 用于IP通信过程中，确认网络是否正常工作，以及遇到异常时进行问题的诊断。 附：《计算机网络-进阶》—ICMP报文消息类型详解 ICMPv6 附：《计算机网络-进阶》—ICMPv6的邻居探索 DHCP 在使用DHCP之前，需先将DHCP所要分配的IP地址池、相应的子网掩码，默认路由控制信息，DNS服务器地址设置到服务器上。 DHCP中继代理 使用DHCP中继代理之后，对不同网段的IP地址的分配也可以由一个DHCP服务器统一进行管理和维护。 路由选择算法距离向量算法(Distance-Vector) 根据网络中的距离(跳数)和方向两种信息生成路由控制表。 RIP▼ 选择经过路由器个数最少的路径 每隔30秒与其他相邻的路由器交换自己的路由表 直接相连的网络距离为0。距离的最大值为16，防止发生“无限计数”问题。 缺点： 网络的个数越多，每次所要交换的路由控制信息就越大。 在网络比较稳定时，还要定期交换信息，浪费贷款。 由于每个路由器掌握的信息都不同，其正确性很难保证。 无法实现可变长度子网构造的网络路由控制（路由器未交换子网掩码）。 RIP2 与RIP第一版的工作机制基本相同，增加了一些新的特点： 链路状态算法(Link-State) 在了解网络整个链接状态的基础上，所有路由器都持有相同的路由控制表。 OSPF▼ 选择总的代价较小的路径。 允许多条费用相同的路径(而RIP仅一条路径) 对每条链路，对不同的TOS(服务类型)，设置多种费用测度 在大规模网络中，用层次的OSPF 附：OSPF的工作机制.png 层次OSPF BGP▼ - 路径向量型协议 边界网关协议是连接不同组织机构(不同自治系统)的一种协议。主要用于ISP之间的相互连接。 根据所要经过的AS路径信息访问列表▼进行路由控制。 AS路径信息访问列表(AS Path List)：包含转发方向、距离以及途径所有AS的编号。 传输层 网络层的IP协议相当于邮递员，把IP数据包送到指定IP的目标主机上， 传输层的TCP/UDP则负责 根据包裹信息（应用程序、端口号），判断信息的最终接收人（哪一个应用程序）。 UDP——用户数据报协议 不提供复杂的控制机制，利用IP提供面向无连接的通信服务。并且它是将应用程序发来的数据在收到的那一刻，立刻按照原样发送到网上的一种机制。 即使出现网络拥堵，UDP也无法进行流量控制等避免网络拥堵的行为。此外，传输途中即使出现丢包，UDP也不负责重发。甚至当出现包的到达顺序乱掉时也没有纠正的功能。如果需要这些细节控制，那么不得不交由采用UDP的应用程序去处理。UDP有点类似于用户说什么听什么的机制，但是需要用户充分考虑好上层协议类型并制作相应的应用程序。因此，也可以说，UDP按照“制作程序的那些用户的指示行事”。 由于UDP面向无连接，它可以随时发送数据。再加上UDP本身处理既简单又高效，因此经常用于以下几个方面： 包总量少的通信（DNS、SNMP等） 视频、音频等多媒体通信（即时通信） 限定于LAN等特定网络中的应用通信 广播通信（广播、多播） 首部格式： 附：UDP数据报首部“校验和”字段详解.png TCP——传输控制协议目的：通过IP数据报实现可靠性传输 特点： 丢包时的重发控制 对次序乱掉的分包进行顺序控制 面向有连接的协议，只有在确认对端存在时才会发送数据，从而可以控制通信流量的浪费 使用序列号和确认应答号实现顺序控制、窗口控制与重发控制。 使用窗口大小来实现对TCP的流量大小的控制 和 拥塞控制 首部格式： 附：TCP数据报首部格式各字段详解.png TCP连接的建立与终止：(传说中的三次握手与四次挥手) 在数据通信之前，主机A通过TCP首部发送一个SYN包作为建立连接的请求（ACK=0,SYN=1，声明一个起始序号seq=x）等待确认应答 主机B收到A要建立连接的请求，返回一个允许连接的SYNACK报文段（ACK=1,SYN=1，主机B的起始序号seq=y，确认应答号ack=x+1） 为防止“已失效的连接请求报文段▼”突然又传回B，主机A再进行一次确认（ACK=1,SYN=0，主机A的起始序列号seq=x+1，确认应答号ack=y+1） 数据传输结束之后，主机A可以发送一个TCP首部（FIN=1，主机A的起始序列号seq=u），请求断开连接。 主机B收到“A不再发送数据的消息”后，返回一个确认报文（ACK=1,seq=v，ack=u+1）A收到B的确认后，进入等待状态，等待B请求释放连接。 B数据发送完成之后，向A请求断开连接（FIN=1,ACK=1,seq=w，ack=u+1） A收到B的断开请求后，回复一个确认信息，并进入TIME_WAIT状态▼，等待2MSL时间。 虽然三次握手方式管理TCP连接可以更好地避免无连接，但这种协议为DOS攻击▼（更确切的说是SYN洪泛攻击▼）提供了可乘之机。 使用序列号和确认应答号实现顺序控制、窗口控制与重发控制 其他传输层协议 UDP-Lite（Lightweight User Datagram Protocol，轻量级用户数据报协议） SCTP（Stream Control Transmission Protocol，流控制传输协议） DCCP（Datagram Congestion Control Protocol，数据报拥塞控制协议） 附：已经步入实用阶段的几个传输层协议详解.png 应用层。。。写不动了 附：几道常考的计算题.pptx 本文中提到的一些专有名词： MAC地址(Media Access Control Address)：介质访问控制地址。系统在网络上的唯一硬件编号，每个网卡都需要有一个唯一的MAC地址。 CSMA/CD(Carrier Sense Mutiple Access/Collision Detection)：具有碰撞检测的载波侦听多路访问 半双工：只发送或只接受的通信方式。比如：无线电收发器，若两端同时说话，是听不见对方说的话的。 全双工：发送数据的同时也可以接收数据。比如：打电话。 STP：Spanning Tree Protocol SFD：Start Frame Delimiter FDDI(Fiber Distribute Data Interface)：分布式光纤数据接口。 ATM(Asynchronous Transfer Mode)：一种以信元为单位的异步传输模式。 网络收敛：网络拥堵时，路由器或交换机发生丢包的现象。 PPPoE(PPP over Ethernet)：利用PPP的验证功能使各家ISP▼有效地管理终端用户的使用，实现按时计费。 ISP:Internet Service Provider NAT：Network Address Translator Path MTU Discovery：发现路径中存在的所有数据链路中最小的MTU▼ MTU：Maximum Transmission Unit。参见《计算机网络-进阶》—路径MTU发现 ARP(Address Resolution Protocol)：地址解析协议 RARP：Reverse Address Resolution Protocol ICMP：Internet Control Managemet Protocol DHCP：Dynamic Host Configuration Protocol RIP：Routing Information Protocol OSPF：Open Shortest Path First BGP(Border Gateway Protocol)：边界网关协议 已失效的连接请求报文段： DOS攻击：deny of service SYN洪泛攻击:攻击者发送大量的TCP SYN报文段，而有意不进行第三次握手的步骤，直至服务器消耗完为第一次握手连接保留的有限资源。 TIME_WAIT状态： 在TIME_WAIT状态中，如果TCP client端最后一次发送的ACK丢失了，它将重新发送。TIME_WAIT状态中所需要的时间是依赖于实现方法的。典型的值为30秒、1分钟和2分钟。等待之后连接正式关闭，并且所有的资源(包括端口号)都被释放。 2MSL(Maximum Segment Lifetime,最大报文生存时间)：虽然按道理，四个报文都发送完毕，我们可以直接进入CLOSE状态了，但是我们必须假想网络是不可靠的，有可能最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的MBP]]></title>
    <url>%2Ftool%2Fmy_mbp%2F</url>
    <content type="text"><![CDATA[必备工具ChromeiTerm2 + Homebrew + zsh Alfred 有道 workflow 剪切板[支持正版，自行购买下载] 万年历App Store免费下载 CleanMyMac彻底卸载App[支持正版，自行购买下载] 效率工具ManicoApp切换App Store免费下载，建议将IntelliJ IDEA加入静默模式 SktechBook画图工具，比Windows里的画图强大100倍。App Store免费下载。 Spectacle小眼镜 分屏工具 Sublime Text 3ctrl + ` 获取package control最新安装代码：https://packagecontrol.io/installation cmd+shift+ppackage control: install packagecmd+shift+ppretty json Preferences —&gt; Key Bindings：123[ &#123; "keys": ["super+i"], "command": "copy_path" &#125;] typora轻量级Markdown编辑神器！ 专业工具Parallels Desktop虚拟机 Java开发环境JDK JDK 1.8：http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html JDK 1.7：http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html JDK 1.6：http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase6-419409.html JDK 1.5：http://www.oracle.com/technetwork/java/javasebusiness/downloads/java-archive-downloads-javase5-419410.html MavenIDEAdownloadlanyus 算法测试Code Templates:12345678910111213141516#if ($&#123;PACKAGE_NAME&#125; &amp;&amp; $&#123;PACKAGE_NAME&#125; != "")package $&#123;PACKAGE_NAME&#125;;#end#parse("File Header.java")import junit.framework.TestCase;import org.junit.Test;/* */public class $&#123;NAME&#125; extends TestCase &#123; @Test public void testCase() &#123; &#125;&#125; XmindVisual Paradigm文本比较工具 DiffMerge 轻量级，容易更改系统文件打开方式 Beyond Compare 重、总崩、需破解 Charles安装证书PC端 “Help” -&gt; “SSL Proxying” -&gt; “Install Charles Root Certificate” 移动端“Help” -&gt; “SSL Proxying” -&gt; “Install Charles Root Certificate on a Mobile Device or Remote Browser” 访问chls.pro/ssl自动下载证书。（此时要确认已经将WiFi的代理模式改为“手动”模式，服务器和端口号填写正确） 如果还是没有自动下载，也可以点击这个链接 `https://www.charlesproxy.com/assets/legacy-ssl/charles.crt直接下载（参考文档：[Legacy SSL Proxying](https://www.charlesproxy.com/documentation/additional/legacy-ssl-proxying/) ） 设置-&gt;通用-&gt;描述文件与设备管理中安装。 设置-&gt;通用-&gt;关于本机-&gt;证书信任设置，开启信任Charles Proxy SSL Proxying 安卓手机也可能不能从服务器下载证书，可以在pc端的 Charles 从 “Help” -&gt; “SSL Proxying” -&gt; “Save Charles Root Certificate”导出后，在手机 设置&gt;安全&gt;安装证书 本地安装。 Https设置白名单“Proxy”-&gt;”SSL Proxying Settings”，添加 *:* 安装chrome的插件Proxy SwitchyOmega，切换到127.0.0.1:8888的代理，即可使用Charles抓本地浏览器的包了。 另一种方法：https://www.charlesproxy.com/documentation/using-charles/ssl-certificates/ 注：由于现在的app都有SSL证书检测机制，用 charles 进行 https 的抓包本质上就是作为一个中间人代理用自己生成的 SSL 证书来完成 https 请求的过程，这样就导致客户端接收到的 SSL 证书其实是一个假的，所以客户端不认这个证书，导致unknown，解决方案：使用一些手段（ROOT / 越狱），是你的要抓的app信任你的Charles证书，或者安装app的debug包。 APP Store精选偶然发现的一些免费小玩意： Maipo for 微博 魔力拼图Lite 五子棋 Gomoku 扫雷 Minesweeper Deluxe 装X工具Aerial桌面屏保 iStat Menus[支持正版，自行购买下载] uBarwin风格的任务栏[支持正版，自行购买下载] Commander One文件管理[支持正版，自行购买下载]]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 常用命令]]></title>
    <url>%2Ftool%2Flinux_command%2F</url>
    <content type="text"><![CDATA[查看大文件进入根目录，查看那个目录的利用率(used)达到了100% df -lh #查看磁盘空间占用情况（每天一个linux命令（33）：df 命令） 进入log目录， du -h #显示当前目录下的文件的磁盘空间使用情况相当于 ls -aslh（每天一个linux命令（34）：du 命令） grephttps://www.cnblogs.com/peida/archive/2012/12/17/2821195.html less每天一个linux命令（13）：less 命令]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 小技巧]]></title>
    <url>%2Ftool%2Fgit_skills%2F</url>
    <content type="text"><![CDATA[启用大小写敏感 git config —global core.ignorecase false （- - global） 批量删除分支git branch | grep &#39;hotfix&#39; | xargs git branch -D git rebase合并多个commit 为1个commit，然后确保本地的git log为最新的时，即可直接git push origin &lt;branch&gt; -f 查看git提交日志： git log —graph —pretty=oneline —abbrev-commit 1234* a5da1fb (HEAD -&gt; master, origin/master) 第4次commit* 0c20956 第3次commit* 230096e 第2次commit* 7bfab4e 第1次commit git rebase（合并重写commit信息）想要合并1-3条，有两个方法： 从HEAD版本开始往过去数3个版本 git rebase -i HEAD~3 指名要合并的版本之前的版本号 git rebase -i 7bfab4e 请注意7bfab4e这个版本是不参与合并的，可以把它当做一个坐标 选取要合并的提交12345678pick 230096e 第2次commitpick 0c20956 第3次commitpick a5da1fb 第4次commit将下面两个pick 手动改成 s或squashpick 230096e 第2次commits 0c20956 第3次commits a5da1fb 第4次commit 保存并退出:wq，然后会弹出一条&quot;This is a combination of 3 commits&quot;的提示，继续保存并退出:wq git add .（如果有冲突，合并冲突后，执行此命令） git rebase —continue（continue前面是两个 - - ） 【放弃本次rebase是：git rebase —abort（abort前面是两个 - -）】 修改合并后的commit信息 git commit —amend push -f 到远程确保本地的git log是最新的 git push origin &lt;branch&gt; -f git reset（代码回滚）git rebase之后，在其他地方git pull代码会和本地的git log有冲突 git pull git reset —hard origin/master（ 注意是 - - hard）（相当于忽略冲突，将项目的版本直接回退到远程仓库的最新版本） 一些示例 git reset —hard = git reset —hard HEAD //相当于放弃当前暂存区的所有修改，恢复到最新的一次cimmit的状态 git reset 7bfab4e //保留当前暂存区的所有修改，并回退到对应的版本位置。（冲突文件以当前暂存区为准） git reset —hard HEAD~3 //改变工作区和暂存区全部重置到 倒数第3个commit版本。该参数等同于重置，可能会引起数据损失。 git revert（撤销某次操作）git revert 撤销某次操作，此次操作之前和之后的commit和history都会保留，并且把这次撤销作为一次最新的提交 git revert HEAD //撤销前一次 commitgit revert HEAD^ //撤销前前一次 commitgit revert fa042ce5 //撤销指定的版本，撤销也会作为一次提交进行保存。 git revert是提交一个新的版本，将需要revert的版本的内容再反向修改回去，版本会递增，不影响之前提交的内容 git stash（代码寄存） git stash [save ‘message’] //将当前所处的工作区/分支的改动保存起来git stash list //保存过的改动记录git stash pop [stash@{0}] //恢复指定缓存（默认最新保存的改动），并删除缓存记录git stash apply [stash@{0}] //恢复指定缓存（默认最新保存的改动），并保留缓存记录git stash drop [stash@{0}] //删除指定缓存（默认最新保存的改动） 统计代码行数git log --author=&quot;username&quot; --pretty=tformat: --numstat | awk &#39;{ add += $1; subs += $2; loc += $1 - $2 } END { printf &quot;added lines: %s, removed lines: %s, total lines: %s\n&quot;, add, subs, loc }&#39; git log --author=&quot;nibnait&quot; --pretty=tformat: --numstat | awk &#39;{ add += $1; subs += $2; loc += $1 - $2 } END { printf &quot;added lines: %s, removed lines: %s, total lines: %s\n&quot;, add, subs, loc }&#39;]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac 小技巧]]></title>
    <url>%2Ftool%2Fmac_skills%2F</url>
    <content type="text"><![CDATA[解压.rar、.7z文件 unrar x 1.rar7z e 1.7z before: brew install unrar brew search 7zbrew install p7zip 坑Updating Homebrew... 法1： 替换brew.git:cd “$(brew —repo)”git remote set-url origin https://mirrors.ustc.edu.cn/brew.git 替换homebrew-core.git:cd “$(brew —repo)/Library/Taps/homebrew/homebrew-core”git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git 法2: 重置brew.git:cd “$(brew —repo)”git remote set-url origin https://github.com/Homebrew/brew.git 重置homebrew-core.git:cd “$(brew —repo)/Library/Taps/homebrew/homebrew-core”git remote set-url origin https://github.com/Homebrew/homebrew-core.git 一些快捷键显示隐藏文件：command + shift + . 彻底删除：command + delete]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 启用 cdn 加速]]></title>
    <url>%2Ftrying%2F83f2d349-cdn%2F</url>
    <content type="text"><![CDATA[参考：https://shup.cn/2017/11/21/cdn.html 分析：哪些因素会影响加载速度？js文件js文件就是JavaScript脚本文件，用来实现各种酷炫的动态效果。一般是加载在html页面的部分，所以页面加载速度首先要考虑这一块。Hexo的js文件大致包含以下3类： 公共js公共js是通用型，比如实现图片放大效果的Fancybox，或者实现图片慢加载的LazyLoad等等。这些js已经开发好，直接在html里引用(存放在镜像网站上，如BootCDN)js文件链接即可实现效果。 定制页面效果js即NexT主题中定制的某些特效，例如实现页面响应式效果的Bootstrap，或者增强移动端滚动及动画效果的Motion等等。这些js理论上也是公共js，只不过NexT主题作者修改了代码，以适用于NexT主题。这些js文件存放在本地Hexo目录hexo\themes\next\source\js\src下。 第三方插件js这个不用多说了吧，Hexo默认安装了不少实用的第三方插件，比如，可直接在网页中显示数学公式的MathJax，优化SEO的Baidu-push等等。这些插件可在主题配置文件_config.yml中进行配置。相关js文件有些存放在hexo\themes\next\layout_third-party中，有些则通过外部引用形式加载。图片(音、视频)如果博客文章很多，绝大部分的带宽都会消耗在这部分静态数据上。这个不多说了。 解决：逐个优化公共js加速修改_config.yml中 vendors 的配置全部映射到BootCDN上去 第三方js文件、图片(音、视频)加速我用的七牛云，每月 10 G的标准存储 CDN 回源流量免费额度https://blog.qiniu.com/archives/8874)。]]></content>
      <categories>
        <category>搞点事情</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>hexo</tag>
        <tag>cdn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 启用 https 加密连接]]></title>
    <url>%2Ftrying%2Fhttps%2F</url>
    <content type="text"><![CDATA[CloudFlare) 提供免费的SSL证书和cdn加速服务。目前还在研究中。]]></content>
      <categories>
        <category>搞点事情</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>hexo</tag>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World Mac版]]></title>
    <url>%2Fmisc%2Fhello_world%2F</url>
    <content type="text"><![CDATA[安装nodehttps://nodejs.org/en/ 测试 npm -vnode -v 安装hexo sudo npm install hexo —savesudo npm install -g hexo 测试 hexo -v 初始化hexo mkdir hexocd hexohexo init Install dependencies时，出现了一些mkdir权限的问题，日志提示要手动npm install sudo npm install hexo generate 在Hexo 3.0 后server被单独出来了，需要安装server sudo npm install hexo-server -savesudo npm install hexo-deployer-git —save 预览 hexo s 发布 hexo d -g 安装调试主题参考： https://www.jianshu.com/p/9f0e90cc32c2 http://eternalzttz.com/hexo-next.html修改网站图标 favicon修改菜单栏 hexo new page “tags” 去/themes/next/source/tags的index.md文件中，添加type: “tags”打开github_banner添加RSS&gt; sudo npm install hexo-generator-feed --save 在/_config.yml 中添加 plugins: hexo-generate-feed 修改底部标签样式修改hexo\themes\next\layout_macro\post.swig中文件，command+f搜索rel=&quot;tag&quot;&gt;#，将#替换成&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;。输入以下命令，查看效果： 侧边栏社交小图标设置social: 网站底部加上访问量不蒜子：https://busuanzi.ibruce.info/ busuanzi_count: enable: true 在themes/next/layout/_partials/footer.swig 适当位置加上：(脚本头部无需添加 ，因为在config.yml中已经开启则无需重复引用) &lt;span id=&quot;busuanzi_container_site_pv&quot;&gt;本站总访问量&lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt;次&lt;/span&gt; 统计博客全站字数 sudo npm install hexo-wordcount —save /themes/next/layout/_partials/footer.swig &lt;div class=&quot;theme-info&quot;&gt; &lt;div class=&quot;powered-by&quot;&gt;&lt;/div&gt; &lt;span class=&quot;post-count&quot;&gt;博客全站共{{ totalcount(site) }}字&lt;/span&gt; &lt;/div&gt; symbols_count_time: item_text_total: true 统计文章字数、阅读时间/themes/next/layout/_macro/post.swig &lt;span class=&quot;post-count&quot;&gt; &amp;nbsp | &amp;nbsp &lt;i class=&quot;fa fa-print&quot;&gt;&lt;/i&gt; 字数统计：{{ wordcount(post.content) }}&lt;/span&gt; &lt;span class=&quot;post-count&quot;&gt; &amp;nbsp | &amp;nbsp &lt;i class=&quot;fa fa-clock&quot;&gt;&lt;/i&gt; 阅读时长≈{{ min2read(post.content, {cn: 300, en: 160}) }}分钟&lt;/span&gt; # Post wordcount display settings # Dependencies: https://github.com/willin/hexo-wordcount post_wordcount: item_text: true wordcount: true # 字数统计 min2read: true # 预计阅读时长 totalcount: true # 总字数统计 separated_meta: true 添加网页顶部进度加载条编辑主题配置文件，command+F搜索pace，将其值改为ture就可以了，选择一款你喜欢的样式。Installation 参考：https://github.com/theme-next/theme-next-pace pace: true 浏览页面的时候显示当前浏览进度 # Scroll percent label in b2t button. scrollpercent: true 添加基于Valine的评论模块valine: enable: true # When enable is set to be true, leancloud_visitors is recommended to be closed for the re-initialization problem within different leancloud adk version. appid: RN3HHH17DaSH4MgBW1bwRIHj-gzGzoHsz appkey: tDXty7nXldylBkVzfPLPnuGU notify: true # mail notifier, See: https://github.com/xCss/Valine/wiki verify: true # Verification code placeholder: ヾﾉ≧∀≦)o来啊，快活啊! # comment box placeholder avatar: mp # gravatar style guest_info: nick,mail,link # custom comment header pageSize: 10 # pagination size language: zh-cn # language, available values: en, zh-cn visitor: false # leancloud-counter-security is not supported for now. When visitor is set to be true, appid and appkey are recommended to be the same as leancloud_visitors&#39; for counter compatibility. Article reading statistic https://valine.js.org/visitor.html comment_count: true # if false, comment count will only be displayed in post page, not in home page vendors: valine: http://tb.nsfocus.co/js/Valine.min_6.js 添加基于Mob ShareSDK的分享功能参考：https://hadronw.com/2018/05-28/hexo-addshares/ 博文置顶 修改hexo-generator-index插件，把node_modules/hexo-generator-index/lib/generator.js中代码替换为： &#39;use strict&#39;; var pagination = require(&#39;hexo-pagination&#39;); module.exports = function(locals){ var config = this.config; var posts = locals.posts; posts.data = posts.data.sort(function(a, b) { if(a.top &amp;&amp; b.top) { // 两篇文章top都有定义 if(a.top == b.top) return b.date - a.date; // 若top值一样则按照文章日期降序排 else return b.top - a.top; // 否则按照top值降序排 } else if(a.top &amp;&amp; !b.top) { // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233） return -1; } else if(!a.top &amp;&amp; b.top) { return 1; } else return b.date - a.date; // 都没定义按照文章日期降序排 }); var paginationDir = config.pagination_dir || &#39;page&#39;; return pagination(&#39;&#39;, posts, { perPage: config.index_generator.per_page, layout: [&#39;index&#39;, &#39;archive&#39;], format: paginationDir + &#39;/%d/&#39;, data: { __index: true } }); }; 文章添加Top值，值越大，越靠前： --- title: Hexo-NexT主题配置 date: 2018-01-20 20:41:08 categories: Hexo tags: - Hexo - NexT top: 100 --- 添加侧栏推荐阅读编辑主题配置文件，如下配置即可： # hexo rolls links_icon: link links_title: 推荐阅读 #links_layout: block links_layout: inline links: Swift 4: https://developer.apple.com/swift/ Objective-C: https://developer.apple.com/documentation/objectivec 修改字体大小编辑hexo/themes/next/source/css/_variables/base.styl，command+F搜索$font-size-base，修改为你想要的大小： // Font size $font-size-base = 16px 在文章底部增加版权信息 在目录hexo/themes/next/layout/_macro/，添加文件 my-copyright.swig，内容如下：my-copyright.swig 在目录hexo/themes/next/source/css/_common/components/post/下添加文件my-post-copyright.styl，添加以下代码：my-post-copyright.styl 修改hexo/themes/next/layout/_macro/post.swig，在如图位置添加以下代码： &lt;div&gt; {% if not is_index %} {% include 'my-copyright.swig' %} {% endif %} &lt;/div&gt; 在hexo/themes/next/source/css/_common/components/post/post.styl文件最后加入下面的代码： @import &quot;my-post-copyright&quot; 在Markdown文章中加入 copyright : ture --- title: Hexo-NexT主题配置 date: 2018-01-20 20:41:08 categories: Hexo tags: - Hexo - NexT top: 100 copyright: ture --- 配置根目录下的_config.yml文件，配置为： # URL ## If your site is put in a subdirectory, set url as &#39;http://yoursite.com/child&#39; and root as &#39;/child/&#39; url: http://tianbin.org root: / permalink: :year/:month/:day/:title/ permalink_defaults: Hexo博客添加站内搜索 NexT主题支持集成 Swiftype、 微搜索、Local Search 和 Algolia。下面介绍Local Search的安装吧。 安装 hexo-generator-search sudo npm install hexo-generator-search —save 安装 hexo-generator-searchdb sudo npm install hexo-generator-searchdb —save 打开local_search local_search: enable: ture 为文章中的代码块增加一键复制功能 copy_button: enable: true 404公益页面 在source下新建404.md --- comments: false --- &lt;script type=&quot;text/javascript&quot; src=&quot;//qzonestyle.gtimg.cn/qzone/hybrid/app/404/search_children.js&quot; homepagename=&#39;返回主页&#39; homepageurl=&quot;/&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; 修改theme/_config.yml menu: commonweal: /404/ || heartbeat 移动端启用侧边栏目录 # Enable sidebar on narrow view (only for Muse | Mist). onmobile: true 打开渲染数学公式开关math: enable: true 写文章时，在文件头添加 mathjax: true 由于Hexo 的 markdown 默认渲染引擎hexo-renderer-marked会把一些’_’、’*’、’{‘、’}’、’\\’等符号转成html标签。 sudo npm uninstall hexo-renderer-marked —save sudo npm install hexo-renderer-kramed —save 修改文件：hexo/node_modules/kramed/lib/rules/inline.js 11行：escape: /^\\([\\`*{}\[\]()#$+\-.!_&gt;])/, 改成：escape: /^\\([`*\[\]()#$+\-.!_&gt;])/, 20行：em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 改成：em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 自动备份Hexo博客源文件参考：备份Hexo博客源文件 安装shelljs模块 sudo npm install —save shelljs 编写脚本文件，内容如下：auto_backup_script.js]]></content>
      <categories>
        <category>杂</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
